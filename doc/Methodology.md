## **Methodology: The PromptOptima Experimental Framework**

This section details the comprehensive methodology designed for the PromptOptima research initiative. It outlines the system architecture, the experimental workflow, the specific instruments for data collection, and the analytical plan for testing our core hypotheses and answering our research questions. The framework is designed to be a robust, replicable system for investigating the nuances of effective prompt engineering in an educational context.

### **1. Research Design & Workflow**

This study employs a **mixed-methods research design**, combining quantitative analysis of user performance metrics with qualitative analysis of LLM-generated text. The primary instrument for data collection is the PromptOptima interactive web application, which facilitates a structured, multi-stage experimental workflow for each user interaction.

The end-to-end workflow proceeds as follows:

1.  **Problem Contextualization:** The user is presented with a 7th-grade math problem and first classifies it using three dropdown menus: `Content Domain`, `Cognitive Demand Level`, and `Problem Context`. This provides essential metadata for later analysis.
2.  **Prompt Submission & Initial Analysis:** The user submits a prompt. The system backend immediately calculates a set of objective linguistic metrics for the prompt text.
3.  **Combined LLM Execution:** The problem context and user prompt are sent in a single, structured API call to a "Solver" LLM (`gpt-3.5-turbo-1106`). The model is instructed to return a single JSON object containing both a step-by-step **solution** and its own **qualitative analysis** of the user's prompt.
4.  **Asynchronous Solution Evaluation:** The generated solution, along with the original problem, is logged for a multi-faceted, asynchronous evaluation process. This includes manual correctness verification and automated analysis of explanation quality and consistency.
5.  **Metric-Driven Feedback & Iteration:** The user is presented with the solution and a real-time dashboard comparing the objective metrics of their prompt with the LLM's estimations. When the user requests an "Improvement Suggestion," the system identifies a weak metric and provides a targeted, randomized prompt template to guide refinement. Each subsequent submission repeats this workflow, creating a rich, longitudinal dataset for each user session.

### **2. Data Collection Instruments & Variables**

#### **2.1. Prompt Quality Metrics (Independent Variables)**

These are objective, deterministic metrics calculated by our backend to quantify the linguistic quality of each user-generated prompt.

* **MATTR (Moving Average Type-Token Ratio):** Measures the lexical diversity and richness of the vocabulary used in the prompt.
* **Reading Ease (LIX-based):** Quantifies the textual complexity. A higher score indicates a prompt that is simpler and more accessible to read.
* **Token Count:** A direct measure of the prompt's length, used to investigate the relationship between verbosity and effectiveness (H2).

#### **2.2. Solution Quality Metrics (Dependent Variables)**

The quality of the LLM's generated solution is the primary dependent variable, measured using a comprehensive, three-part system.

* **Metric 1: Answer Correctness (Manual Verification):** A binary score (1 for Correct, 0 for Incorrect) assigned by a human researcher. This manual approach is chosen to ensure accurate evaluation across all problem types, including conceptual questions where automated tests are not feasible.
* **Metric 2: Explanation Quality (AI Judge Rubric):** A score out of 16 generated by a powerful "Evaluator" LLM (`gpt-4o`) acting as an expert teacher. It assesses the solution's explanation against a predefined rubric with four criteria: *Logical Soundness, Step Completeness, Calculation Accuracy,* and *Pedagogical Clarity*.
* **Metric 3: Consistency Check:** A score from 0 to 1 measuring the model's reliability, determined by running the same prompt three times and comparing the final numerical answers.

#### **2.3. Contextual and Behavioral Variables**

* **User-Defined Context:** `content_domain`, `cognitive_level`, `problem_context`.
* **Post-Facto Prompt Label:** A manually assigned label (`Level 0-3`) categorizing the structure of the user's prompt, essential for RQ1.
* **LLM-Generated Rationale:** The `analysis_rationale` text from the LLM, providing qualitative insight for RQ2.
* **User Progression Data:** `user_id`, `session_id`, `timestamp`, and `suggestion_shown` are logged to track user journeys and measure the impact of feedback for RQ3.

### **3. Model Implementation & Rationale**

* **Solver Model (`gpt-3.5-turbo-1106`):** This model serves as the primary subject of our study. Its widespread availability and support for structured JSON output make it an ideal baseline for generating findings relevant to a broad audience.
* **Evaluator Model (`gpt-4o`):** GPT-4o is employed exclusively for the evaluation task in Metric 2. This is a critical methodological choice to mitigate bias. By using a separate, more advanced model for evaluation, we avoid "self-grading" and leverage GPT-4o's superior reasoning and instruction-following capabilities to ensure the scoring rubric is applied with the highest possible fidelity.

### **4. Data Analysis Plan**

The collected data will be analyzed to test our hypotheses and answer the research questions.

#### **4.1. The Composite Quality Score (CQS)**

To create a single, unified measure of solution quality, the three solution metrics are normalized to a 0-1 scale and combined using a weighted formula. The weights prioritize correctness (50%), followed by explanation quality (35%), and consistency (15%). The final score is presented on a 10-point scale.
`CQS = ((Score_Norm1 * 0.5) + (Score_Norm2 * 0.35) + (Score_Norm3 * 0.15)) * 10`

#### **4.2. Hypothesis Testing**

* **To Test H1 (Prompt Quality â†’ Solution Quality):** We will perform a correlation analysis. A `Prompt_Quality_Score` will be calculated by combining the normalized MATTR and Reading Ease scores. We will then create a **scatter plot** of `Prompt_Quality_Score` (X-axis) vs. `CQS` (Y-axis) and compute the **Pearson correlation coefficient (r)**. A strong, positive `r` value will support this hypothesis.

* **To Test H2 (Optimal Structure):** We will create a **scatter plot** of `Token_Count` (X-axis) vs. `CQS` (Y-axis). If the hypothesis is correct, the plot will show that CQS does not increase indefinitely with token count, likely peaking and then plateauing or declining. This will be supplemented by comparing the average CQS for prompts with different post-facto `prompt_label` levels.

#### **4.3. Answering the Research Questions**

* **To Answer RQ1 (Efficacy of Structures):** We will segment the entire dataset by the user-defined context variables (e.g., `content_domain`). Within each segment, we will group the runs by their post-facto `prompt_label` (Level 0-3) and calculate the average `CQS` for each group. The results will be presented in **bar charts** to visually compare the effectiveness of different structures for specific problem types.

* **To Answer RQ2 (LLM's Perspective):** We will conduct a **qualitative thematic analysis** of the `analysis_rationale` text collected from all runs. We will identify recurring themes, categorize them, and calculate their frequency. The findings will be visualized using **word clouds** and **frequency charts** to reveal the factors the LLM consistently associates with high-quality prompts.

* **To Answer RQ3 (Feedback Effectiveness):** We will perform a **longitudinal analysis** of user journeys. By grouping data by `user_id`, we will track the evolution of their `Prompt_Quality_Score` over time. We will specifically compare the scores of prompts created immediately before and after a `suggestion_shown` event. The aggregate percentage improvement will be calculated and visualized using **line charts** (for individual journeys) and **comparative bar charts** (for overall impact).